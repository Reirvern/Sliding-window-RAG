{
    "chunking": {
        "chunk_size": 3000,
        "overlap_size": 100,
        "chunk_by": "sentences",
        "keep_sentences_together": true,
        "encoding": "utf-8",
        "language": "ru",
        "min_chunk_size": 50,
        "model_name": null
    },
    "retrieval": {
        "strategy_type": 3,
        "model_path": "models/retrieval_model.gguf",
        "keywords": ["Alt-RAG", "проект", "архитектура"],
        "top_k": 3000,
        "retriever_prompt": "проанализируй отрывок ниже, и дай ответ в форме да/нет на вопрос '{prompt}' Не фантазируй, отвечай 'да' только в том случае если абсолютно точно уверен в своем ответе! Ответ дать одним словом, либо 'да', либо 'нет'.\n\n {chunk_content}\n\nОтвет:",
        "retriever_fallback_prompt": "Проанализируй следующий фрагмент текста и строго ответь 'да' или 'нет' на вопрос '{prompt}'. Ответ должен быть ТОЛЬКО 'да' или 'нет'.\n\nФрагмент: {chunk_content}\n\nОтвет: "
    },
    "synthesis": {
        "strategy_type": 1,
        "synthesis_prompt": "Используя следующий контекст:\n{context}\n\nОтветь на вопрос: {question}\n\nПожалуйста, после каждого утверждения, которое основывается на контексте, укажи точную цитату из контекста в формате [ЦИТАТА: \"...\"]. Если информация не найдена в контексте, ответь, что информация отсутствует.",
        "context_token_buffer": 2000
    },
    "retrieval_inference": {
        "engine_type": "llamacpp",
        "model_path": "models/mlabonne_gemma-3-4b-it-abliterated-Q8_0.gguf",
        "n_gpu_layers": 30,
        "device_type": "gpu",
        "n_ctx": 6096, 
        "temperature": 0.1, 
        "max_new_tokens": 50, 
        "top_p": 0.90,
        "top_k": 20,
        "repeat_penalty": 1.0,
        "stop_sequences": []
    },
    "retrieval_fallback_inference": {
        "engine_type": "llamacpp",
        "model_path": "models/mlabonne_gemma-3-12b-it-abliterated-Q4_K_M.gguf",
        "n_gpu_layers": 26,
        "device_type": "gpu",
        "n_ctx": 4096,
        "temperature": 0.01,
        "max_new_tokens": 5,
        "top_p": 0.1,
        "top_k": 1,
        "repeat_penalty": 1.0,
        "stop_sequences": ["\n", ".", ",", "!", "?", "Да.", "Нет."]
    },
    "synthesis_inference": {
        "engine_type": "llamacpp",
        "model_path": "models/mlabonne_gemma-3-4b-it-abliterated-Q8_0.gguf",
        "n_gpu_layers": 34,
        "device_type": "gpu",
        "n_ctx": 16384,
        "temperature": 0.7,
        "max_new_tokens": 500,
        "top_p": 0.95,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "stop_sequences": []
    },
    "general_language": "ru"
}
